# Video "what is TensorFlow"

%config IPCompleter.greedy=True

import tensorflow as tf
import numpy as np
print(ts.__version__)

# output: 1.3.0

tf.reset_default_graph()
a = tf.placeholder(np.float32, (2,2))
b = tf.Variable(tf.ones((2,2)))
c = a @ b

print(c)

# output: Tensor("matmul:0", shape=(2,2), dtype = float32)

s = tf.InteractiveSession()

s.run(tf.global_variables_initializer())
s.run(c, feed_dict = {a: np.ones((2,2))})

# output: array([[2., 2.,],[2., 2.]], dtype = float32)

s.close()

# ------------------------------------------------------------------------------------
# Video "first model in TensorFlow"
# Simple optimization (with simple prints)  ----- (1)

tf.reset_default_graph()
x = tf.get_variable("x", shape=(), dtype = tf.float32, trainable = True)

optimizer = tf.train.GradientDescentOptimizer(0.1)
step = optimizer.minimize(f, var_list[x])

tf.trainable_variables()

# output: [<tf.Variable 'x:0' shape() dtype = float32>]

with tf.Session() as s:                # in this way session will be closed automatically
  s.run(tf.gloable_variables_initializer())
  for i in range(10):
    _, curr_x, curr_f = s.run([step, x, f])
    print(curr_x, curr_f)
    
# Simple optimization (with tf.Print) ---- (2)

tf.reset_default_graph()
x = tf.get_variable("x", shape=(), dtype=tf.float32)
f = x**2
f = tf.Print(f, [x,f], "x,f: ")

optimizer = tf.train.GradientDescentOptimizer(0.1)
step = optimizer.minize(f)

with tf.Session() as s:
  s.run(tf.global_variables_initializer())
  for i in range(10):
    s.run([step, f])
    
# Simple optimization ( with TensorBoard logging)  --- (3)

tf.reset_default_graph()
x = tf.get_variable("x", shape = (), dtype = tf.float32)
f = x**2

optimizer = tf.train.GradientDescentOptimizer(0.1)
step = optimizer.minizer(f)

tf.summary.scalar('curr_x', x)
tf.summary.scalar('curr_f', f)
summaries = tf.summary.merge_all()

s = tf.InteractiveSession()
summary_writer = tf.summary.FileWriter("logs/1", s.graph)
s.run(tf.globe_variables_initializer())
for i in range(10):
  _, curr_summaries = s.run([step, summaries])
  summary_writer.add_summary(curr_summaries, i)
  summary_writer.flush()
  
 # Run tensorboard --logdir=./logs in bash
 # If you're running on Google Colab you can still run TensorBoard!
 
 # !!! RUN THIS CELL ONLY ON GOOGLE COLAB !!!
! wget https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/setup_google_colab.py -O setup_google_colab.py
import setup_google_colab

# run tensorboard in background
import os
os.system("tensorboard --logdir=./logs --host 0.0.0.0 --port 6006 &")

# expose port and show the link
setup_google_colab.expose_port_on_colab(6006)
 
s.close()

# --------------------------------------------------------------------------------------------------------------------
# Training a linear model 

# generate model data
N = 1000
D = 3
x = np.random.random((N, D))
w = np.random.random((D, 1))
y = x @ w + np.random.randn(N, 1) * 0.20

print(x.shape, y.shape)
print(w.T)

tf.reset_default_graph()

features = tf.placeholder(tf.float32, shape=(None, D))
target = tf.placeholder(tf.float32, shape=(None, 1))

weights = tf.get_variable("weights", shape=(D, 1), dtype=tf.float32)
predictions = features @ weights

loss = tf.reduce_mean((target - predictions) ** 2)

print(target.shape, predictions.shape, loss.shape)

optimizer = tf.train.GradientDescentOptimizer(0.1)
step = optimizer.minimize(loss)

with tf.Session() as s:
    s.run(tf.global_variables_initializer())
    for i in range(300):
        _, curr_loss, curr_weights = s.run([step, loss, weights], 
                                           feed_dict={features: x, target: y})
        if i % 50 == 0:
            print(curr_loss)
           
# found weights
curr_weights.T

# true weights
w.T




